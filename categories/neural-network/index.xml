<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Neural-NetWork on ticks blog</title>
    <link>https://tickscn.github.io/categories/neural-network/</link>
    <description>Recent content in Neural-NetWork on ticks blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Wed, 23 Dec 2020 15:52:00 +0800</lastBuildDate><atom:link href="https://tickscn.github.io/categories/neural-network/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>池化层和 BatchNorm 层</title>
      <link>https://tickscn.github.io/post/tf_pooling_layer/</link>
      <pubDate>Wed, 23 Dec 2020 15:52:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/tf_pooling_layer/</guid>
      <description>池化层 在卷积的时候可以通过设置步长来将输入等效缩小, 实际上有一种专门的网络层可以用来实现尺寸缩小, 即 池化层(Pooling Layer). 常用的有 最大池化</description>
    </item>
    
    <item>
      <title>tensorflow 实现卷积层</title>
      <link>https://tickscn.github.io/post/tensorflow_layers_convolution/</link>
      <pubDate>Wed, 23 Dec 2020 15:02:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/tensorflow_layers_convolution/</guid>
      <description>自定义权值 在 tensorflow 中通过 tf.nn.conv2d 函数可以实现 2 维卷积. 需要设置输入, 卷积和, 步长, 边缘填充 卷积层类 layers.Conv2D</description>
    </item>
    
    <item>
      <title>卷积神经网络</title>
      <link>https://tickscn.github.io/post/convolution_neural_net/</link>
      <pubDate>Tue, 22 Dec 2020 16:04:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/convolution_neural_net/</guid>
      <description>多通道输入和单卷积核 单通道输入只需要将卷积核平移. 卷积核的通道数与输入相同, 每个通道单独运算之后结果相加. 多通道多卷积核 多个卷积核卷积之后结</description>
    </item>
    
    <item>
      <title>全连接网络的问题</title>
      <link>https://tickscn.github.io/post/nn_all_problems/</link>
      <pubDate>Tue, 22 Dec 2020 15:35:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/nn_all_problems/</guid>
      <description>参数过多, 假设网络参数为 float32, 一个参数占用内存 4B. 网络参数有 \(b((n_1n_2 + n_2*n_3+\cdots)+\sum n_i\) 当输入为 28*28 的图片, 批数为 2. 网络层数为 4. 输出长度为 256, 256, 64, 32, 10 需要的内存为 (2*(784*256+256*256+256*64+64*32+32*10)+256+256+64+32+10)/1024/1024 MiB=0.544MiB 当在</description>
    </item>
    
    <item>
      <title>Dropout 与数据增强</title>
      <link>https://tickscn.github.io/post/ml_gropout/</link>
      <pubDate>Mon, 21 Dec 2020 16:33:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/ml_gropout/</guid>
      <description>dropout 随机的断开网络中的某些连接, 也可以缩小结果空间, 减小过拟合 可以使用 tf.nn.dropout(x, rate=) 将连接以 rate 的概率断开 也可以将其作为一个网络层 network.add(layers.Dropout(rate=)) 数据增强 增加数据集规模</description>
    </item>
    
    <item>
      <title>正则化</title>
      <link>https://tickscn.github.io/post/ml_regular/</link>
      <pubDate>Sat, 19 Dec 2020 16:27:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/ml_regular/</guid>
      <description>对于多项式约束来说, 当参数稀疏化后, 表达能力可以将低, 为了减小过拟合, 对目标函数进行修改. 不仅要求损失函数尽量小, 还要求参数尽量稀疏. \[\mathtt{min} \mathcal{L}(f_{\theta},y)+\lambda\Omega(\theta), (\bf{x},</description>
    </item>
    
    <item>
      <title>数据集划分</title>
      <link>https://tickscn.github.io/post/ml_pdata_split/</link>
      <pubDate>Sat, 19 Dec 2020 16:01:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/ml_pdata_split/</guid>
      <description>数据集一般都有训练集与测试集, 为了选择超参数和检测过拟合现象, 一般又将训练集再分为普通的训练集和验证集. 验证集 为了在训练中, 测试网络的超参数</description>
    </item>
    
    <item>
      <title>机器学习模型的容量</title>
      <link>https://tickscn.github.io/post/ml_model_space/</link>
      <pubDate>Sat, 19 Dec 2020 15:42:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/ml_model_space/</guid>
      <description>机器学习是为了用网络来逼近真实模型, 这样才可以去预测实际中的输入. 这需要学习用的样品和实际用例是独立同分布的. 而且为了更好的拟和需要对网络的</description>
    </item>
    
    <item>
      <title>可视化</title>
      <link>https://tickscn.github.io/post/tf_viewable/</link>
      <pubDate>Fri, 18 Dec 2020 20:09:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/tf_viewable/</guid>
      <description>tensorflow 提供了一个远程监控网络训练进度的库 TensorBoard 模型端 在网络模型端需要建立一个数据监控类 1 summary_writer = tf.summary.create_file_write(log_dir) 标量数据可以通过 tf.summary.scalar(&#39;label&#39;, float(v), step=s) 以间隔 s 将数据 v 写入集合 label 中. TensorBoard</description>
    </item>
    
    <item>
      <title>tensorflow 提供的网络</title>
      <link>https://tickscn.github.io/post/tensorflow_applications/</link>
      <pubDate>Fri, 18 Dec 2020 19:56:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/tensorflow_applications/</guid>
      <description>对于常用网络, 如 ResNet 或 VGG 等, 不需要手动创建网络, 只需要从 keras.applications 子模块中通过一行代码即可创建并使用这些经典模型, 也可以通过设置权重 weights 参数加载预训练的</description>
    </item>
    
    <item>
      <title>自定义网络层</title>
      <link>https://tickscn.github.io/post/tf_define_layers/</link>
      <pubDate>Fri, 18 Dec 2020 16:30:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/tf_define_layers/</guid>
      <description>虽然 tensorflow 已经提供了很多的网络层类, 但是对于深度学习来说还不够. 对于需要自定义逻辑的网络层, 可以通过自定义类来实现, 网络层需要继承 layers.Layer. 网络需要继承</description>
    </item>
    
    <item>
      <title>模型保存与加载</title>
      <link>https://tickscn.github.io/post/tf_module_save_load/</link>
      <pubDate>Fri, 18 Dec 2020 16:17:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/tf_module_save_load/</guid>
      <description>模型的训练与加载非常重要, 可以用来保存训练的网络, 也可以保存训练中间的快照, 方便重启运算 张量的方式 网络的状态包括网络结构和网络中张量数据, 当</description>
    </item>
    
    <item>
      <title>tensorflow 模型装配, 训练与测试</title>
      <link>https://tickscn.github.io/post/tf_module_build_train_test/</link>
      <pubDate>Fri, 18 Dec 2020 16:06:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/tf_module_build_train_test/</guid>
      <description>模型装配 Keras 中有两个特殊的类, keras.Module 和 keras.layers.Layer. 前者定义了网络的母类, 具有保存模型, 加载模型, 训练与测试 生面网络参数网络训练时需要前向计算和误差计算, 反传</description>
    </item>
    
    <item>
      <title>神经网络误差计算</title>
      <link>https://tickscn.github.io/post/neural_network_loss/</link>
      <pubDate>Sun, 13 Dec 2020 19:18:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/neural_network_loss/</guid>
      <description>常见的误差函数有均方差, 交叉熵, KL 散射, Hinge Loss 函数 均方差 均方差(Mean Square Error, MSE) \[\mathrm{MSE}(\boldsymbol{\widetilde{y},y}) = \frac{1}{N}\sum_i (\widetilde{y}_i-y_i)^2\] 可以使用计算函数 loss = keras.losses.MSE(y_onehot, o), loss=tf.reduce_mean(loss) 也可以使用层的方法 1 2 criteon = keras.losses.MeanSquareError() loss</description>
    </item>
    
    <item>
      <title>神经网络输出层设计</title>
      <link>https://tickscn.github.io/post/neural_network_output_layer/</link>
      <pubDate>Sun, 13 Dec 2020 18:24:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/neural_network_output_layer/</guid>
      <description>常见的输出类型 实数空间. [0,1]区间 \(y_{i}\in [0,1], \sum_{i}y_{i}=1\) [-1,1] 区间输出层的设计应该与实际的应用相结合 普通的实数空间 输出层不使用激活函数, 直接将输出 \(\tilde{y}\) 与真实值</description>
    </item>
    
    <item>
      <title>神经网络激活函数</title>
      <link>https://tickscn.github.io/post/tf_activation/</link>
      <pubDate>Sat, 12 Dec 2020 16:18:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/tf_activation/</guid>
      <description>Sigmoid \[\mathrm{Sigmoid}(x)=\frac{1}{1+\mathrm{e}^{-x}}\] 可以将输入压缩到 (0,1). 是递增函数且连续可导. 但是在输入值较大或较小时梯度变化较小, 被称为梯度弥散. 使训练不收敛或停滞不动 ReLU \[\mathrm{relu}(x) = \begin{cases}0, &amp;amp;x\leqslant 0\\x,&amp;amp;x&amp;gt;0\end{cases}\] LeakyReLU 当 x&amp;lt;0 时</description>
    </item>
    
    <item>
      <title>机器学习-网络</title>
      <link>https://tickscn.github.io/post/tf_perceptron/</link>
      <pubDate>Sat, 12 Dec 2020 16:13:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/tf_perceptron/</guid>
      <description>感知机 \[a=\sigma(z)=\sigma(\boldsymbol{w^Tx}+b)\] 其中 \(\bf{w}\) 为权值, b 为偏置. \(\sigma\) 是激活函数. 激活函数可以是阶感知机模型无法解决异或等线性不可分问题, 需要嵌套多层神经网络 全连接层 感知机模型</description>
    </item>
    
    <item>
      <title>Tensorflow 复杂一些的功能函数</title>
      <link>https://tickscn.github.io/post/tensorflow_complex_func/</link>
      <pubDate>Fri, 11 Dec 2020 20:01:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/tensorflow_complex_func/</guid>
      <description>tf.gather() 收集用的函数. 如想要收集图片集合 x [1000, 32, 32, 3] 中的某几张, 可以使用 tf.gather(x, [1,3,4,...], axis=0). 如果收集的序号是相临的多个, 使用切片也许更好 tf.gather_nd 通过指定采样点的多维坐标</description>
    </item>
    
    <item>
      <title>tensorflow 张量限制</title>
      <link>https://tickscn.github.io/post/tensorflow_fill_copy/</link>
      <pubDate>Fri, 11 Dec 2020 19:24:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/tensorflow_fill_copy/</guid>
      <description>填充 因为输入数据的原始值可能有各不相同的长度, 为了方便并行计算, 将不同的长度扩展为相同的长度. 之前可以通过 tf.tile 复制数据, 但这会破坏原有的数据结</description>
    </item>
    
    <item>
      <title>tensorflow 张量比较</title>
      <link>https://tickscn.github.io/post/tensorflow_tensorcomp/</link>
      <pubDate>Fri, 11 Dec 2020 16:38:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/tensorflow_tensorcomp/</guid>
      <description>可以使用 tf.equal(a,b) 比较两个张量, 返回值是表示各元素是否相等的布尔张量. 将其转为 int 型并求和可以得出相等的元素个数. 在最后统计准确率时很好用测试一下随机</description>
    </item>
    
    <item>
      <title>tensorflow 数据统计</title>
      <link>https://tickscn.github.io/post/tensorflow_statistic/</link>
      <pubDate>Wed, 09 Dec 2020 17:05:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/tensorflow_statistic/</guid>
      <description>常见的统计信息, 如最值和最值的位置, 均值, 范数等信息 向量范数 L1 范数所有元素的绝对值之和 \[\Vert \boldsymbol{x}\Vert_1 = \sum_i \vert x_{i}\vert\] L2 范数所有元素平方和, 再开平方根 \[\Vert \boldsymbol{x}\Vert_2 = \sqrt{\sum_i x_i^2}\]</description>
    </item>
    
    <item>
      <title>tensorflow 张量合并与分割</title>
      <link>https://tickscn.github.io/post/tensorflow_concat_split/</link>
      <pubDate>Wed, 09 Dec 2020 15:59:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/tensorflow_concat_split/</guid>
      <description>合并 将多个张量在茉个维度上合并为一个张量. 如有不同来源的集合 A [1000, 28, 28] 和 B [2000, 28, 28]. 可能希望将 A,B 合并为一个张量 C [3000,28,28]. 合并可以使用拼接(concate</description>
    </item>
    
    <item>
      <title>tensotflow 数学运算</title>
      <link>https://tickscn.github.io/post/tensorflow_math_op/</link>
      <pubDate>Tue, 08 Dec 2020 16:41:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/tensorflow_math_op/</guid>
      <description>加减乘除 tf 中的函数为 tf.add, tf.subtract, tf.multiply, tf.divide. 不过 tensorflow 已重载了 +,-,*,/ 运算符. 可以直接使用运算符. 同时也重载了整除 //和求模%运算符 乘方(冪) 函数为 tf.pow() 或使用重载的运算</description>
    </item>
    
    <item>
      <title>tensorflow 维度变换</title>
      <link>https://tickscn.github.io/post/tensorflow_dims_trans/</link>
      <pubDate>Tue, 08 Dec 2020 16:17:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/tensorflow_dims_trans/</guid>
      <description>改这视图 reshape 操作用来改变张量的视图. 将相同的存储使用不同的解读方法. 所以合法的视图转换操作要求可以不改变存储形式来实现 例: 初始张量 \(\bf{A}\) 按初始视图</description>
    </item>
    
    <item>
      <title>tensorflow 数据类型</title>
      <link>https://tickscn.github.io/post/tensorflow_base_type/</link>
      <pubDate>Tue, 08 Dec 2020 15:41:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/tensorflow_base_type/</guid>
      <description>数值类型 标量(Scalar): 单个的实数, 维度为 0. shape 为[] 向量(Vector): n 个实数的有序集合. shape 为[n] 矩阵(Matrix): n 行 m 列</description>
    </item>
    
    <item>
      <title>分类问题</title>
      <link>https://tickscn.github.io/post/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/</link>
      <pubDate>Fri, 04 Dec 2020 18:18:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/</guid>
      <description>预测值是离散的, 这种问题被称为分类问题. 比如之前举出的判断给出点的象限的问题 模型构建 单输入, 单输出 \(y = wx+b\) 多输入, 单输出 \(y = \boldsymbol{w}^T\boldsymbol{x}+b\) 多输入, 多输出 \(\boldsymbol{y</description>
    </item>
    
    <item>
      <title>回归问题</title>
      <link>https://tickscn.github.io/post/%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/</link>
      <pubDate>Thu, 03 Dec 2020 20:37:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/</guid>
      <description>一个线性问题 \(y = wx+b\). 如果有两个点, 可以直接求出解析解. 如果是很多个点, 求一个 误差 最小的直线需要怎么作? 首先定义误差为直线采样点与样本点之间的差</description>
    </item>
    
    <item>
      <title>神经网络简要介绍</title>
      <link>https://tickscn.github.io/post/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA%E5%85%A5%E9%97%A8/</link>
      <pubDate>Thu, 03 Dec 2020 16:39:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%90%86%E8%AE%BA%E5%85%A5%E9%97%A8/</guid>
      <description>机器学习 机器学习可以分为监督学习(Supervised Learning), 无监督学习(Unsupervised Learning), 强化学习(Reinforcement Learning, RL) 有监督</description>
    </item>
    
    <item>
      <title>实现一个简单的网络</title>
      <link>https://tickscn.github.io/post/dualnet_example/</link>
      <pubDate>Wed, 02 Dec 2020 16:48:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/dualnet_example/</guid>
      <description>代码 将之前的介绍用 python 实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58</description>
    </item>
    
    <item>
      <title>反向传播介绍</title>
      <link>https://tickscn.github.io/post/neural_backpropagation/</link>
      <pubDate>Tue, 01 Dec 2020 20:54:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/neural_backpropagation/</guid>
      <description>链式法则 当某个复合函数满足 外部函数有连续偏导数 内部函数可偏导 则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示。 反向传播 加法结点</description>
    </item>
    
    <item>
      <title>入门示例</title>
      <link>https://tickscn.github.io/post/neural_introduction/</link>
      <pubDate>Tue, 01 Dec 2020 16:54:00 +0800</pubDate>
      
      <guid>https://tickscn.github.io/post/neural_introduction/</guid>
      <description>一个简单的分类任务。已知四个数据点 \((1,1),(-1,1),(-1,-1),(1,-1)\), 分别属于 I \(\sim\) IV 象限. 如果给一个新的点， 如何让机器知道它应该属于哪个象限呢？因为特征简单，传统方法，使用两</description>
    </item>
    
    <item>
      <title>Keras 常见功能模块</title>
      <link>https://tickscn.github.io/post/tf_keras_normal_module/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://tickscn.github.io/post/tf_keras_normal_module/</guid>
      <description>Keras 是一个高层的神经网络接口, 可以使用其它神经网络框架作为后端, 而 Tensorflow 实现了自己的 Keras 接口, tf.keras. 结合更紧密, 配合更好. 它的常见功能模块有 网络层类 对于</description>
    </item>
    
    <item>
      <title>浏览器端</title>
      <link>https://tickscn.github.io/post/deep_learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://tickscn.github.io/post/deep_learning/</guid>
      <description>使用命令 1 tensorboard --logdir path</description>
    </item>
    
  </channel>
</rss>
